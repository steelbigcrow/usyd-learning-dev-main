基于对各模块的学术研究，我们进入了开发阶段。

为了深入理解模糊逻辑（FL）流程，并在将学术研究成果转化为测试平台组件的过程中及时验证各组件的正确性，我们通过模拟客户端库的架构搭建了一个本地测试平台。

参数加载与解析

参数加载：在 src/utils/config.py 文件中，我们设置了 `--arch-config` 和 `--train-config` 参数。`--arch-config` 定义了模型架构，`--train-config` 定义了模糊逻辑的超参数。这些参数是通过调用 Python 解析器包的 `add_argument` 方法，并经由 `build_argparser` 函数加载的。

参数解析：在 scripts/fed_train.py 文件中，主函数调用 `parser.parse_args()` 函数解析用户输入的文件路径。

非独立同分布模拟

5.2.1 标签偏斜极端情况

准备阶段完成后，我们将得到一个名为 cfg 的字典，其中包含整个过程的配置信息。

在此步骤中，我们模拟联邦学习中的标签偏斜场景。首先，我们检索 cfg['dataset'] 中的值，并调用相应的数据加载函数（例如 get_mnist_datasets）来获取完整的数据集 train_ds。然后，我们将此数据集 train_ds 传递给数据划分函数（例如 partition_mnist_label_shift(train_ds, num_clients=num_clients)），将其划分为非独立同分布。这将生成一个名为 parts 的字典，其中键是客户端 ID，值是客户端的数据。

5.2.2 双重不平衡情况

在 skewed_longtail_noniid 中，我们重点模拟同时存在样本偏斜和标签偏斜的双重不平衡情况。具体来说，这是样本偏斜分布（例如，客户 1 有 10 条数据，而客户 9 有 100 条数据）和标签偏斜分布（例如，客户 1 只有标签 0，而客户 9 的标签从 0 到 9）的综合影响。

在 skewed_longtail_noniid/demonstration.py 文件中，我们构建了一个字典，用于定义此场景下客户的预期数据量和标签分布。具体来说，该字典结构包含一个外键、一个内键和一个内值。外键定义客户 ID，内键定义标签，内值表示客户标签的相对比例。例如，0: {0: 10, 1: 10} 表示客户端 0 的标签 0 和标签 1 的相对比例均为 10。

在 skewed_longtail_spec.py 文件中，我们使用 to_dense_weights 函数将分布规范转换为一个矩阵，其中每一行代表一个客户端，每一列代表标签的数量。然后，我们使用 normalize_weights_to_counts 函数来实现标签分布。在分布过程中，我们使用最大余数法。首先，我们根据标签的相对比例将其分配为整数。对于小数部分，我们每次取最大余数，并将对应的标签计数加 1，直到标签分布完成。

在 skewed_longtail_partitioner.py 文件中，我们加载数据集并随机打乱每个标签的索引。这可以确保客户端获取的数据特征是随机的，并根据 skewed_longtail_spec.py 中建立的分布字典来分配标签。模型工厂

在本模块中，我们使用工厂模式来实现面向对象编程 (OOP) 中的开闭原则。这样，如果将来需要向测试平台添加更高级的模型，我们可以在 `src/models/` 目录下添加一个新文件来定义模型架构，并在 `src/models/registry.py` 中注册新增的模型。这样，新的模型架构就成功集成到了我们的测试平台中。

通过配置文件，我们传入所选模型，注册表会调用 `create_model()` 函数来初始化相应的模型结构，从而成功实现解耦。

如果不使用这种设计模式，那么当需要添加更多模型架构时，就必须修改客户端的初始化代码，添加一个 if 语句来判断需要初始化的模型，然后传入相应的参数。在这种情况下，模型架构将与客户端高度耦合。

参数-高效微调技术

首先，根据 `--arch-config` 路径指向的 YAML 文件确定要初始化的模型。然后，根据 `cfg` 字典中的 `use_lora` 或 `use_adalora` 选项执行相应的 PEFT 算法。最后，将预训练模型参数加载到已初始化的模型架构中。

LoRA

超参数设置。进入 LoRA 算法分支后，首先从 `cfg` 中读取 LoRA 配置文件。

层替换。然后调用 `inject_lora_modules` 函数，传入 LoRA 超参数和预训练的基础模型 `global_model`。
n 调用 src/training/lora_utils.py 中的 _replace_modules_with_lora 函数。该函数递归遍历 global_model 中的每一层，以确定它是否是需要替换的 LoRA 层。如果是，则使用 loralib 库中的 lora.Linear 或 lora.Embedding 创建一个替换层。

参数训练。lora 层包含两个额外的成员变量 self.lora_A 和 self.lora_B。它们分别是 LoRA 中的可训练参数 A 和 B。被替换层的权重和偏置作为主路径传递给 lora 层，而 self.lora_A 和 self.lora_B 则作为旁路路径。这使得 lora 层能够替换基础模型中的目标层。

AdaLoRA

层替换。如果需要替换的层是线性层，我们将使用 loralib.adalora.SVDLinear 来替换基础模型的线性层。由于 loralib.adalora 没有 Embedding 函数，因此在处理此分支时，我们将不再使用 AdaLoRA，而是使用 lora 中的 Embedding 层进行替换。

可训练矩阵初始化。SVDLinear 层首先将 USV 的三个可训练矩阵初始化为成员变量，然后将基础模型中线性层的权重复制到 SVDLinear 层。因此，SVDLinear 层由基础模型权重作为主路径，USV 作为旁路路径组成。

动态排序管理。通过配置文件获取 AdaLoRA 的超参数，包括初始排序、目标排序、预热期（在此之前不执行任何剪枝操作）、剪枝周期（执行剪枝操作的轮数）和微调周期（在执行剪枝操作的轮数之后不再执行剪枝操作）。

`scripts/fed_train.py` 中的 `create_rank_allocator` 函数会根据上述配置的参数创建一个 `RankAllocator`。它可以自适应地在不同层之间分配参数预算。

异构 LoRA 的聚合方法

LoRA 零填充聚合

服务器端聚合

获取所有客户端每一层的 LoRA 矩阵，并找到最大行数 (max_rows) 和最大列数 (max_cols)。

它使用 torch.zeros 创建一个具有 max_rows 行和 max_cols 列的空白矩阵。然后，将每个客户端的矩阵复制到该空白矩阵中。

现在维度相同了，它对这些矩阵进行加权平均，以获得全局更新矩阵。

客户端自适应

对于 LoRA_A 矩阵，截断前 r 行并保留所有列。对于 LoRA_B 矩阵，保留所有行并截断前 r 列。

基于奇异值分解的 LoRa 聚合

服务器端聚合

提取每一层的秩，并使用 W_delta = B_i @ A_i 将其重构为一个完整大小的矩阵。此步骤解决了客户端 i 之间 LoRa 矩阵大小不同的问题。

在服务器端使用以下步骤聚合秩：W_sum = W_sum + (w * W_delta) 和 W_g = W_sum / total_w。

使用奇异值分解重新分解全局更新矩阵，并使用 LoRAUtils.svd_split 方法从客户端提取最大秩 rmax。

使用平方根将奇异值矩阵拆分为两边，以使矩阵中元素的量级更加接近。

Bglobal=U 且 Aglobal=V

客户端适配

由于全局矩阵分解是根据客户端的最大秩进行的，因此客户端需要根据自身的秩 ri 截断矩阵 Bglobal 和 Aglobal 的大小。由于奇异值矩阵按重要性排序，因此最终保留前 ri 行，因为它们包含了矩阵中尽可能多的信息。
