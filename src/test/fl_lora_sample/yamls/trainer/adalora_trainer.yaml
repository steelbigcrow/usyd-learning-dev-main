trainer:
  trainer_type: "adalora"
  deivce: "cpu"
  save_path: "./.training_results/"
  # AdaLoRA configuration (fields forwarded to peft.AdaLoraConfig)
  adalora:
    # Base LoRA-style knobs（提高初始/目标秩以增加容量）
    r: 16            # 基础秩/最大秩上限（不变）
    init_r: 8        # 初始秩
    target_r: 6      # 目标秩（训练后期的期望秩）
    lora_alpha: 16
    lora_dropout: 0.05
    total_step: 1000
    # Optional AdaLoRA extras (all optional; uncomment/tune as needed)
    # tinit: 0
    # tfinal: 0
    # deltaT: 1
    # beta1: 0.85
    # beta2: 0.85
    # orth_reg_weight: 0.5
    # bias: "none"            # one of: none, all, lora_only
    # inference_mode: false
    # use_rslora: false
    # use_dora: false
    # rank_pattern: {}
    # target_modules: ["linear1", "linear2"]
